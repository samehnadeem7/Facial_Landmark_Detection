{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNxQVLcFzbB1JiVQKl053cF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"chq4c8o7Hh66"},"outputs":[],"source":["import numpy as np\n","!pip install idx2numpy\n","import idx2numpy\n","import numpy as np\n","data_path = '/content/drive/MyDrive/Colab Notebooks/archive/'\n","train_images_path = data_path + 'train-images.idx3-ubyte'\n","train_labels_path = data_path + 'train-labels.idx1-ubyte'\n","test_images_path = data_path + 't10k-images.idx3-ubyte'\n","test_labels_path = data_path + 't10k-labels.idx1-ubyte'\n","\n","X_train = idx2numpy.convert_from_file(train_images_path)\n","y_train = idx2numpy.convert_from_file(train_labels_path)\n","\n","\n","X_test = idx2numpy.convert_from_file(test_images_path)\n","y_test = idx2numpy.convert_from_file(test_labels_path)\n","\n","\n","X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0\n","X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0\n","\n","\n","def one_hot_encode(y, num_classes=10):\n","    return np.eye(num_classes)[y].T\n","\n","Y_train = one_hot_encode(y_train)\n","Y_test = one_hot_encode(y_test)\n","\n","def softmax(z):\n","\n","    max_z = np.max(z, axis=0, keepdims=True)\n","    exp_z = np.exp(z - max_z)\n","    softmax_output = exp_z / np.sum(exp_z, axis=0, keepdims=True)\n","    return softmax_output\n","\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def relu(z):\n","    return np.maximum(0, z)\n","\n","def layer_size(X, Y):\n","    n_x = X.shape[0]\n","    n_h = 4\n","    n_y = Y.shape[0]\n","    return (n_x, n_h, n_y)\n","\n","def initialize_parameters(n_x, n_h, n_y):\n","    W1 = np.random.randn(n_h, n_x) * 0.01\n","    b1 = np.zeros((n_h, 1))\n","    W2 = np.random.randn(n_y, n_h) * 0.01\n","    b2 = np.zeros((n_y, 1))\n","\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    return parameters\n","\n","def forward_propagation(X, parameters):\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","\n","    Z1 = np.dot(W1, X) + b1\n","    A1 = np.tanh(Z1)\n","    Z2 = np.dot(W2, A1) + b2\n","    A2 = softmax(Z2)\n","\n","    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n","    return A2, cache\n","\n","def compute_cost(A2, Y):\n","    m = Y.shape[1]\n","    log_probs = -np.sum(Y * np.log(A2 + 1e-8)) / m\n","    return log_probs\n","\n","def backward_propagation(parameters, cache, X, Y):\n","    m = X.shape[1]\n","    W1 = parameters['W1']\n","    W2 = parameters['W2']\n","    A1 = cache['A1']\n","    A2 = cache['A2']\n","\n","    dZ2 = A2 - Y\n","    dW2 = np.dot(dZ2, A1.T) / m\n","    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n","    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n","    dW1 = np.dot(dZ1, X.T) / m\n","    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n","\n","    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n","    return grads\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","\n","    W1 = W1 - learning_rate * grads['dW1']\n","    b1 = b1 - learning_rate * grads['db1']\n","    W2 = W2 - learning_rate * grads['dW2']\n","    b2 = b2 - learning_rate * grads['db2']\n","\n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    return parameters\n","\n","def predict(X, parameters):\n","    A2, _ = forward_propagation(X, parameters)\n","    return np.argmax(A2, axis=0)\n","\n","def neural_network(X, Y, num_iterations, learning_rate):\n","    n_x = X.shape[0]\n","    n_h = 64\n","    n_y = 10\n","\n","    parameters = initialize_parameters(n_x, n_h, n_y)\n","    costs = []\n","\n","    for i in range(num_iterations):\n","        A2, cache = forward_propagation(X, parameters)\n","        cost = compute_cost(A2, Y)\n","        grads = backward_propagation(parameters, cache, X, Y)\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","\n","        if i % 100 == 0:\n","            print(f\"Cost after iteration {i}: {cost}\")\n","            costs.append(cost)\n","\n","    return parameters, costs\n"]},{"cell_type":"code","source":["\n","num_iterations = 2500\n","learning_rate = 0.05\n","trained_parameters, training_costs = neural_network(X_train, Y_train, num_iterations, learning_rate)\n","\n"],"metadata":{"id":"vHJdXaVHH-o7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_predictions = predict(X_test, trained_parameters)\n","accuracy = np.mean(test_predictions == y_test)\n","print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n","import matplotlib.pyplot as plt\n","\n","plt.plot(range(0, num_iterations, 100), training_costs)\n","plt.xlabel('Iterations')\n","plt.ylabel('Cost')\n","plt.title('Training Cost')\n","plt.show()"],"metadata":{"id":"4HWVxB9IIHNU"},"execution_count":null,"outputs":[]}]}